nohup: ignoring input
Start training ...
ESPnet is not installed, cannot use espnet_hubert upstream
[ INFO : 2024-09-25 10:32:40,402 ] - exp_dir is: exp/240925_ECAPA_TDNN_GLOB_c512_ASTP_emb192_WavLM_Large_frozen_num_frms150_aug06_spTrue_saFalse_ArcMargin_intertopk_subcenter_SGD_e150
[ INFO : 2024-09-25 10:32:40,402 ] - <== Passed Arguments ==>
[ INFO : 2024-09-25 10:32:40,403 ] - {'data_type': 'shard',
[ INFO : 2024-09-25 10:32:40,403 ] -  'dataloader_args': {'batch_size': 512,
[ INFO : 2024-09-25 10:32:40,403 ] -                      'drop_last': True,
[ INFO : 2024-09-25 10:32:40,403 ] -                      'num_workers': 16,
[ INFO : 2024-09-25 10:32:40,404 ] -                      'pin_memory': False,
[ INFO : 2024-09-25 10:32:40,404 ] -                      'prefetch_factor': 16},
[ INFO : 2024-09-25 10:32:40,404 ] -  'dataset_args': {'aug_prob': 0.6,
[ INFO : 2024-09-25 10:32:40,404 ] -                   'cmvn': True,
[ INFO : 2024-09-25 10:32:40,404 ] -                   'cmvn_args': {'norm_mean': True, 'norm_var': False},
[ INFO : 2024-09-25 10:32:40,404 ] -                   'filter': True,
[ INFO : 2024-09-25 10:32:40,404 ] -                   'filter_args': {'max_num_frames': 400, 'min_num_frames': 50},
[ INFO : 2024-09-25 10:32:40,404 ] -                   'frontend': 's3prl',
[ INFO : 2024-09-25 10:32:40,404 ] -                   'num_frms': 150,
[ INFO : 2024-09-25 10:32:40,404 ] -                   'resample_rate': 16000,
[ INFO : 2024-09-25 10:32:40,404 ] -                   's3prl_args': {'download_dir': './s3prl_hub',
[ INFO : 2024-09-25 10:32:40,404 ] -                                  'frame_length': 20,
[ INFO : 2024-09-25 10:32:40,404 ] -                                  'frame_shift': 20,
[ INFO : 2024-09-25 10:32:40,404 ] -                                  'frozen': True,
[ INFO : 2024-09-25 10:32:40,404 ] -                                  'layer': -1,
[ INFO : 2024-09-25 10:32:40,404 ] -                                  'multilayer_feature': True,
[ INFO : 2024-09-25 10:32:40,404 ] -                                  'upstream_args': {'name': 'wavlm_large'}},
[ INFO : 2024-09-25 10:32:40,404 ] -                   'sample_num_per_epoch': 0,
[ INFO : 2024-09-25 10:32:40,404 ] -                   'shuffle': True,
[ INFO : 2024-09-25 10:32:40,404 ] -                   'shuffle_args': {'shuffle_size': 2500},
[ INFO : 2024-09-25 10:32:40,404 ] -                   'spec_aug': False,
[ INFO : 2024-09-25 10:32:40,404 ] -                   'spec_aug_args': {'max_f': 8,
[ INFO : 2024-09-25 10:32:40,404 ] -                                     'max_t': 10,
[ INFO : 2024-09-25 10:32:40,404 ] -                                     'num_f_mask': 1,
[ INFO : 2024-09-25 10:32:40,404 ] -                                     'num_t_mask': 1,
[ INFO : 2024-09-25 10:32:40,404 ] -                                     'prob': 0.6},
[ INFO : 2024-09-25 10:32:40,404 ] -                   'speed_perturb': True},
[ INFO : 2024-09-25 10:32:40,405 ] -  'enable_amp': True,
[ INFO : 2024-09-25 10:32:40,405 ] -  'exp_dir': 'exp/240925_ECAPA_TDNN_GLOB_c512_ASTP_emb192_WavLM_Large_frozen_num_frms150_aug06_spTrue_saFalse_ArcMargin_intertopk_subcenter_SGD_e150',
[ INFO : 2024-09-25 10:32:40,405 ] -  'gpus': [0],
[ INFO : 2024-09-25 10:32:40,405 ] -  'log_batch_interval': 100,
[ INFO : 2024-09-25 10:32:40,405 ] -  'loss': 'CrossEntropyLoss',
[ INFO : 2024-09-25 10:32:40,405 ] -  'loss_args': {},
[ INFO : 2024-09-25 10:32:40,405 ] -  'margin_scheduler': 'MarginScheduler',
[ INFO : 2024-09-25 10:32:40,405 ] -  'margin_update': {'final_margin': 0.2,
[ INFO : 2024-09-25 10:32:40,405 ] -                    'fix_start_epoch': 40,
[ INFO : 2024-09-25 10:32:40,405 ] -                    'increase_start_epoch': 20,
[ INFO : 2024-09-25 10:32:40,405 ] -                    'increase_type': 'exp',
[ INFO : 2024-09-25 10:32:40,405 ] -                    'initial_margin': 0.0,
[ INFO : 2024-09-25 10:32:40,405 ] -                    'update_margin': True},
[ INFO : 2024-09-25 10:32:40,406 ] -  'model': 'ECAPA_TDNN_GLOB_c512',
[ INFO : 2024-09-25 10:32:40,406 ] -  'model_args': {'embed_dim': 192, 'feat_dim': -1, 'pooling_func': 'ASTP'},
[ INFO : 2024-09-25 10:32:40,406 ] -  'model_init': None,
[ INFO : 2024-09-25 10:32:40,406 ] -  'noise_data': '/scratch/users/astar/bmsi/liut1/data/VoxForWe/musan/lmdb',
[ INFO : 2024-09-25 10:32:40,406 ] -  'num_avg': 10,
[ INFO : 2024-09-25 10:32:40,406 ] -  'num_epochs': 150,
[ INFO : 2024-09-25 10:32:40,406 ] -  'optimizer': 'SGD',
[ INFO : 2024-09-25 10:32:40,406 ] -  'optimizer_args': {'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0001},
[ INFO : 2024-09-25 10:32:40,406 ] -  'projection_args': {'easy_margin': False,
[ INFO : 2024-09-25 10:32:40,406 ] -                      'project_type': 'arc_margin_intertopk_subcenter',
[ INFO : 2024-09-25 10:32:40,406 ] -                      'scale': 32.0},
[ INFO : 2024-09-25 10:32:40,406 ] -  'reverb_data': '/scratch/users/astar/bmsi/liut1/data/VoxForWe/rirs/lmdb',
[ INFO : 2024-09-25 10:32:40,406 ] -  'save_epoch_interval': 5,
[ INFO : 2024-09-25 10:32:40,406 ] -  'scheduler': 'ExponentialDecrease',
[ INFO : 2024-09-25 10:32:40,407 ] -  'scheduler_args': {'final_lr': 1e-05,
[ INFO : 2024-09-25 10:32:40,407 ] -                     'initial_lr': 0.1,
[ INFO : 2024-09-25 10:32:40,407 ] -                     'warm_from_zero': True,
[ INFO : 2024-09-25 10:32:40,407 ] -                     'warm_up_epoch': 6},
[ INFO : 2024-09-25 10:32:40,407 ] -  'seed': 42,
[ INFO : 2024-09-25 10:32:40,407 ] -  'train_data': '/scratch/users/astar/bmsi/liut1/data/VoxForWe/vox2_dev/shard.list',
[ INFO : 2024-09-25 10:32:40,407 ] -  'train_label': '/scratch/users/astar/bmsi/liut1/data/VoxForWe/vox2_dev/utt2spk'}
[ INFO : 2024-09-25 10:32:45,359 ] - <== Data statistics ==>
[ INFO : 2024-09-25 10:32:45,359 ] - train data num: 1092009, spk num: 5994
[ INFO : 2024-09-25 10:32:45,895 ] - <== Dataloaders ==>
[ INFO : 2024-09-25 10:32:45,895 ] - train dataloaders created
[ INFO : 2024-09-25 10:32:45,895 ] - epoch iteration number: 2132
[ INFO : 2024-09-25 10:32:45,895 ] - <== Model ==>
[ INFO : 2024-09-25 10:32:45,896 ] - Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt
[ DEBUG : 2024-09-25 10:32:45,896 ] - Attempting to acquire lock 23367925615584 on s3prl_hub/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt.lock
[ DEBUG : 2024-09-25 10:32:45,897 ] - Lock 23367925615584 acquired on s3prl_hub/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt.lock
[ DEBUG : 2024-09-25 10:32:45,897 ] - Attempting to release lock 23367925615584 on s3prl_hub/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt.lock
[ DEBUG : 2024-09-25 10:32:45,897 ] - Lock 23367925615584 released on s3prl_hub/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt.lock
[ INFO : 2024-09-25 10:32:45,898 ] - Using URL's local file: s3prl_hub/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt
[ INFO : 2024-09-25 10:32:47,804 ] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[ INFO : 2024-09-25 10:32:54,746 ] - speaker_model size: 324060505
[ INFO : 2024-09-25 10:32:54,746 ] - Train model from scratch ...
[ INFO : 2024-09-25 10:32:54,796 ] - ECAPA_TDNN(
[ INFO : 2024-09-25 10:32:54,796 ] -   (layer1): Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,796 ] -     (conv): Conv1d(1024, 512, kernel_size=(5,), stride=(1,), padding=(2,))
[ INFO : 2024-09-25 10:32:54,796 ] -     (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,796 ] -   )
[ INFO : 2024-09-25 10:32:54,797 ] -   (layer2): SE_Res2Block(
[ INFO : 2024-09-25 10:32:54,797 ] -     (se_res2block): Sequential(
[ INFO : 2024-09-25 10:32:54,797 ] -       (0): Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,797 ] -         (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,797 ] -         (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,797 ] -       )
[ INFO : 2024-09-25 10:32:54,797 ] -       (1): Res2Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,797 ] -         (convs): ModuleList(
[ INFO : 2024-09-25 10:32:54,797 ] -           (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
[ INFO : 2024-09-25 10:32:54,797 ] -           (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
[ INFO : 2024-09-25 10:32:54,797 ] -           (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
[ INFO : 2024-09-25 10:32:54,797 ] -           (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
[ INFO : 2024-09-25 10:32:54,797 ] -           (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
[ INFO : 2024-09-25 10:32:54,797 ] -           (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
[ INFO : 2024-09-25 10:32:54,797 ] -           (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
[ INFO : 2024-09-25 10:32:54,797 ] -         )
[ INFO : 2024-09-25 10:32:54,797 ] -         (bns): ModuleList(
[ INFO : 2024-09-25 10:32:54,797 ] -           (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,797 ] -           (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,797 ] -           (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,797 ] -           (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,797 ] -           (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,797 ] -           (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,797 ] -           (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,797 ] -         )
[ INFO : 2024-09-25 10:32:54,797 ] -       )
[ INFO : 2024-09-25 10:32:54,798 ] -       (2): Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,798 ] -         (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,798 ] -         (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,798 ] -       )
[ INFO : 2024-09-25 10:32:54,798 ] -       (3): SE_Connect(
[ INFO : 2024-09-25 10:32:54,798 ] -         (linear1): Linear(in_features=512, out_features=128, bias=True)
[ INFO : 2024-09-25 10:32:54,798 ] -         (linear2): Linear(in_features=128, out_features=512, bias=True)
[ INFO : 2024-09-25 10:32:54,798 ] -       )
[ INFO : 2024-09-25 10:32:54,798 ] -     )
[ INFO : 2024-09-25 10:32:54,798 ] -   )
[ INFO : 2024-09-25 10:32:54,798 ] -   (layer3): SE_Res2Block(
[ INFO : 2024-09-25 10:32:54,798 ] -     (se_res2block): Sequential(
[ INFO : 2024-09-25 10:32:54,798 ] -       (0): Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,798 ] -         (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,798 ] -         (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,798 ] -       )
[ INFO : 2024-09-25 10:32:54,798 ] -       (1): Res2Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,798 ] -         (convs): ModuleList(
[ INFO : 2024-09-25 10:32:54,798 ] -           (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
[ INFO : 2024-09-25 10:32:54,798 ] -           (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
[ INFO : 2024-09-25 10:32:54,798 ] -           (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
[ INFO : 2024-09-25 10:32:54,798 ] -           (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
[ INFO : 2024-09-25 10:32:54,798 ] -           (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
[ INFO : 2024-09-25 10:32:54,798 ] -           (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
[ INFO : 2024-09-25 10:32:54,798 ] -           (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
[ INFO : 2024-09-25 10:32:54,798 ] -         )
[ INFO : 2024-09-25 10:32:54,798 ] -         (bns): ModuleList(
[ INFO : 2024-09-25 10:32:54,799 ] -           (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -           (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -           (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -           (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -           (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -           (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -           (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -         )
[ INFO : 2024-09-25 10:32:54,799 ] -       )
[ INFO : 2024-09-25 10:32:54,799 ] -       (2): Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,799 ] -         (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,799 ] -         (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -       )
[ INFO : 2024-09-25 10:32:54,799 ] -       (3): SE_Connect(
[ INFO : 2024-09-25 10:32:54,799 ] -         (linear1): Linear(in_features=512, out_features=128, bias=True)
[ INFO : 2024-09-25 10:32:54,799 ] -         (linear2): Linear(in_features=128, out_features=512, bias=True)
[ INFO : 2024-09-25 10:32:54,799 ] -       )
[ INFO : 2024-09-25 10:32:54,799 ] -     )
[ INFO : 2024-09-25 10:32:54,799 ] -   )
[ INFO : 2024-09-25 10:32:54,799 ] -   (layer4): SE_Res2Block(
[ INFO : 2024-09-25 10:32:54,799 ] -     (se_res2block): Sequential(
[ INFO : 2024-09-25 10:32:54,799 ] -       (0): Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,799 ] -         (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,799 ] -         (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,799 ] -       )
[ INFO : 2024-09-25 10:32:54,799 ] -       (1): Res2Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,799 ] -         (convs): ModuleList(
[ INFO : 2024-09-25 10:32:54,800 ] -           (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
[ INFO : 2024-09-25 10:32:54,800 ] -           (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
[ INFO : 2024-09-25 10:32:54,800 ] -           (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
[ INFO : 2024-09-25 10:32:54,800 ] -           (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
[ INFO : 2024-09-25 10:32:54,800 ] -           (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
[ INFO : 2024-09-25 10:32:54,800 ] -           (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
[ INFO : 2024-09-25 10:32:54,800 ] -           (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
[ INFO : 2024-09-25 10:32:54,800 ] -         )
[ INFO : 2024-09-25 10:32:54,800 ] -         (bns): ModuleList(
[ INFO : 2024-09-25 10:32:54,800 ] -           (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,800 ] -           (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,800 ] -           (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,800 ] -           (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,800 ] -           (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,800 ] -           (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,800 ] -           (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,800 ] -         )
[ INFO : 2024-09-25 10:32:54,800 ] -       )
[ INFO : 2024-09-25 10:32:54,800 ] -       (2): Conv1dReluBn(
[ INFO : 2024-09-25 10:32:54,800 ] -         (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,800 ] -         (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,800 ] -       )
[ INFO : 2024-09-25 10:32:54,800 ] -       (3): SE_Connect(
[ INFO : 2024-09-25 10:32:54,800 ] -         (linear1): Linear(in_features=512, out_features=128, bias=True)
[ INFO : 2024-09-25 10:32:54,800 ] -         (linear2): Linear(in_features=128, out_features=512, bias=True)
[ INFO : 2024-09-25 10:32:54,800 ] -       )
[ INFO : 2024-09-25 10:32:54,800 ] -     )
[ INFO : 2024-09-25 10:32:54,801 ] -   )
[ INFO : 2024-09-25 10:32:54,801 ] -   (conv): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,801 ] -   (pool): ASTP(
[ INFO : 2024-09-25 10:32:54,801 ] -     (linear1): Conv1d(4608, 128, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,801 ] -     (linear2): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))
[ INFO : 2024-09-25 10:32:54,801 ] -   )
[ INFO : 2024-09-25 10:32:54,801 ] -   (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
[ INFO : 2024-09-25 10:32:54,801 ] -   (linear): Linear(in_features=3072, out_features=192, bias=True)
[ INFO : 2024-09-25 10:32:54,801 ] -   (bn2): Identity()
[ INFO : 2024-09-25 10:32:54,801 ] -   (frontend): S3prlFrontend(
[ INFO : 2024-09-25 10:32:54,801 ] -     (upstream): S3PRLUpstream(
[ INFO : 2024-09-25 10:32:54,801 ] -       (upstream): UpstreamExpert(
[ INFO : 2024-09-25 10:32:54,801 ] -         (model): WavLM(
[ INFO : 2024-09-25 10:32:54,801 ] -           (feature_extractor): ConvFeatureExtractionModel(
[ INFO : 2024-09-25 10:32:54,801 ] -             (conv_layers): ModuleList(
[ INFO : 2024-09-25 10:32:54,801 ] -               (0): Sequential(
[ INFO : 2024-09-25 10:32:54,801 ] -                 (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
[ INFO : 2024-09-25 10:32:54,801 ] -                 (1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,801 ] -                 (2): Sequential(
[ INFO : 2024-09-25 10:32:54,801 ] -                   (0): TransposeLast()
[ INFO : 2024-09-25 10:32:54,801 ] -                   (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,801 ] -                   (2): TransposeLast()
[ INFO : 2024-09-25 10:32:54,801 ] -                 )
[ INFO : 2024-09-25 10:32:54,801 ] -                 (3): GELU(approximate='none')
[ INFO : 2024-09-25 10:32:54,801 ] -               )
[ INFO : 2024-09-25 10:32:54,801 ] -               (1): Sequential(
[ INFO : 2024-09-25 10:32:54,801 ] -                 (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
[ INFO : 2024-09-25 10:32:54,801 ] -                 (1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,802 ] -                 (2): Sequential(
[ INFO : 2024-09-25 10:32:54,802 ] -                   (0): TransposeLast()
[ INFO : 2024-09-25 10:32:54,802 ] -                   (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,802 ] -                   (2): TransposeLast()
[ INFO : 2024-09-25 10:32:54,802 ] -                 )
[ INFO : 2024-09-25 10:32:54,802 ] -                 (3): GELU(approximate='none')
[ INFO : 2024-09-25 10:32:54,802 ] -               )
[ INFO : 2024-09-25 10:32:54,802 ] -               (2): Sequential(
[ INFO : 2024-09-25 10:32:54,802 ] -                 (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
[ INFO : 2024-09-25 10:32:54,802 ] -                 (1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,802 ] -                 (2): Sequential(
[ INFO : 2024-09-25 10:32:54,802 ] -                   (0): TransposeLast()
[ INFO : 2024-09-25 10:32:54,802 ] -                   (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,802 ] -                   (2): TransposeLast()
[ INFO : 2024-09-25 10:32:54,802 ] -                 )
[ INFO : 2024-09-25 10:32:54,802 ] -                 (3): GELU(approximate='none')
[ INFO : 2024-09-25 10:32:54,802 ] -               )
[ INFO : 2024-09-25 10:32:54,802 ] -               (3): Sequential(
[ INFO : 2024-09-25 10:32:54,802 ] -                 (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
[ INFO : 2024-09-25 10:32:54,802 ] -                 (1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,802 ] -                 (2): Sequential(
[ INFO : 2024-09-25 10:32:54,802 ] -                   (0): TransposeLast()
[ INFO : 2024-09-25 10:32:54,802 ] -                   (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,802 ] -                   (2): TransposeLast()
[ INFO : 2024-09-25 10:32:54,802 ] -                 )
[ INFO : 2024-09-25 10:32:54,802 ] -                 (3): GELU(approximate='none')
[ INFO : 2024-09-25 10:32:54,802 ] -               )
[ INFO : 2024-09-25 10:32:54,803 ] -               (4): Sequential(
[ INFO : 2024-09-25 10:32:54,803 ] -                 (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
[ INFO : 2024-09-25 10:32:54,803 ] -                 (1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,803 ] -                 (2): Sequential(
[ INFO : 2024-09-25 10:32:54,803 ] -                   (0): TransposeLast()
[ INFO : 2024-09-25 10:32:54,803 ] -                   (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,803 ] -                   (2): TransposeLast()
[ INFO : 2024-09-25 10:32:54,803 ] -                 )
[ INFO : 2024-09-25 10:32:54,803 ] -                 (3): GELU(approximate='none')
[ INFO : 2024-09-25 10:32:54,803 ] -               )
[ INFO : 2024-09-25 10:32:54,803 ] -               (5): Sequential(
[ INFO : 2024-09-25 10:32:54,803 ] -                 (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
[ INFO : 2024-09-25 10:32:54,803 ] -                 (1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,803 ] -                 (2): Sequential(
[ INFO : 2024-09-25 10:32:54,803 ] -                   (0): TransposeLast()
[ INFO : 2024-09-25 10:32:54,803 ] -                   (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,803 ] -                   (2): TransposeLast()
[ INFO : 2024-09-25 10:32:54,803 ] -                 )
[ INFO : 2024-09-25 10:32:54,803 ] -                 (3): GELU(approximate='none')
[ INFO : 2024-09-25 10:32:54,803 ] -               )
[ INFO : 2024-09-25 10:32:54,803 ] -               (6): Sequential(
[ INFO : 2024-09-25 10:32:54,803 ] -                 (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
[ INFO : 2024-09-25 10:32:54,803 ] -                 (1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,803 ] -                 (2): Sequential(
[ INFO : 2024-09-25 10:32:54,803 ] -                   (0): TransposeLast()
[ INFO : 2024-09-25 10:32:54,803 ] -                   (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,803 ] -                   (2): TransposeLast()
[ INFO : 2024-09-25 10:32:54,804 ] -                 )
[ INFO : 2024-09-25 10:32:54,804 ] -                 (3): GELU(approximate='none')
[ INFO : 2024-09-25 10:32:54,804 ] -               )
[ INFO : 2024-09-25 10:32:54,804 ] -             )
[ INFO : 2024-09-25 10:32:54,804 ] -           )
[ INFO : 2024-09-25 10:32:54,804 ] -           (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,804 ] -           (dropout_input): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,804 ] -           (dropout_features): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,804 ] -           (encoder): TransformerEncoder(
[ INFO : 2024-09-25 10:32:54,804 ] -             (pos_conv): Sequential(
[ INFO : 2024-09-25 10:32:54,804 ] -               (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
[ INFO : 2024-09-25 10:32:54,804 ] -               (1): SamePad()
[ INFO : 2024-09-25 10:32:54,804 ] -               (2): GELU(approximate='none')
[ INFO : 2024-09-25 10:32:54,804 ] -             )
[ INFO : 2024-09-25 10:32:54,804 ] -             (layers): ModuleList(
[ INFO : 2024-09-25 10:32:54,804 ] -               (0): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,804 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,804 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,804 ] -                   (relative_attention_bias): Embedding(320, 16)
[ INFO : 2024-09-25 10:32:54,804 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,804 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,804 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,804 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,804 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,804 ] -                 )
[ INFO : 2024-09-25 10:32:54,804 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,804 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,805 ] -               )
[ INFO : 2024-09-25 10:32:54,805 ] -               (1): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,805 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,805 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,805 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                 )
[ INFO : 2024-09-25 10:32:54,805 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,805 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,805 ] -               )
[ INFO : 2024-09-25 10:32:54,805 ] -               (2): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,805 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,805 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,805 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                 )
[ INFO : 2024-09-25 10:32:54,806 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,806 ] -               )
[ INFO : 2024-09-25 10:32:54,806 ] -               (3): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,806 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,806 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                 )
[ INFO : 2024-09-25 10:32:54,806 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,806 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,807 ] -               )
[ INFO : 2024-09-25 10:32:54,807 ] -               (4): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,807 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,807 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                 )
[ INFO : 2024-09-25 10:32:54,807 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,807 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,807 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,807 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,807 ] -               )
[ INFO : 2024-09-25 10:32:54,807 ] -               (5): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,807 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,807 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,807 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                 )
[ INFO : 2024-09-25 10:32:54,808 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,808 ] -               )
[ INFO : 2024-09-25 10:32:54,808 ] -               (6): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,808 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,808 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,808 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                 )
[ INFO : 2024-09-25 10:32:54,808 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,808 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,808 ] -               )
[ INFO : 2024-09-25 10:32:54,808 ] -               (7): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,808 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,809 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                 )
[ INFO : 2024-09-25 10:32:54,809 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,809 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,809 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,809 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,809 ] -               )
[ INFO : 2024-09-25 10:32:54,809 ] -               (8): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,809 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,809 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,809 ] -                 )
[ INFO : 2024-09-25 10:32:54,809 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,809 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,809 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,810 ] -               )
[ INFO : 2024-09-25 10:32:54,810 ] -               (9): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,810 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,810 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,810 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                 )
[ INFO : 2024-09-25 10:32:54,810 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,810 ] -               )
[ INFO : 2024-09-25 10:32:54,810 ] -               (10): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,810 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,810 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,810 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,810 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                 )
[ INFO : 2024-09-25 10:32:54,811 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,811 ] -               )
[ INFO : 2024-09-25 10:32:54,811 ] -               (11): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,811 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,811 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,811 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                 )
[ INFO : 2024-09-25 10:32:54,811 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,811 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,811 ] -               )
[ INFO : 2024-09-25 10:32:54,812 ] -               (12): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,812 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,812 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                 )
[ INFO : 2024-09-25 10:32:54,812 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,812 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,812 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,812 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,812 ] -               )
[ INFO : 2024-09-25 10:32:54,812 ] -               (13): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,812 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,812 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,812 ] -                 )
[ INFO : 2024-09-25 10:32:54,812 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,813 ] -               )
[ INFO : 2024-09-25 10:32:54,813 ] -               (14): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,813 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,813 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,813 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                 )
[ INFO : 2024-09-25 10:32:54,813 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,813 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,813 ] -               )
[ INFO : 2024-09-25 10:32:54,813 ] -               (15): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,813 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,813 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,813 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                 )
[ INFO : 2024-09-25 10:32:54,814 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,814 ] -               )
[ INFO : 2024-09-25 10:32:54,814 ] -               (16): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,814 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,814 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                 )
[ INFO : 2024-09-25 10:32:54,814 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,814 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,815 ] -               )
[ INFO : 2024-09-25 10:32:54,815 ] -               (17): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,815 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,815 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                 )
[ INFO : 2024-09-25 10:32:54,815 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,815 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,815 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,815 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,815 ] -               )
[ INFO : 2024-09-25 10:32:54,815 ] -               (18): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,815 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,815 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,815 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                 )
[ INFO : 2024-09-25 10:32:54,816 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,816 ] -               )
[ INFO : 2024-09-25 10:32:54,816 ] -               (19): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,816 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,816 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,816 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                 )
[ INFO : 2024-09-25 10:32:54,816 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,816 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,816 ] -               )
[ INFO : 2024-09-25 10:32:54,816 ] -               (20): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,816 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,817 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                 )
[ INFO : 2024-09-25 10:32:54,817 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,817 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,817 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,817 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,817 ] -               )
[ INFO : 2024-09-25 10:32:54,817 ] -               (21): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,817 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,817 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,817 ] -                 )
[ INFO : 2024-09-25 10:32:54,817 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,817 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,817 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,818 ] -               )
[ INFO : 2024-09-25 10:32:54,818 ] -               (22): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,818 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,818 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,818 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                 )
[ INFO : 2024-09-25 10:32:54,818 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,818 ] -               )
[ INFO : 2024-09-25 10:32:54,818 ] -               (23): TransformerSentenceEncoderLayer(
[ INFO : 2024-09-25 10:32:54,818 ] -                 (self_attn): MultiheadAttention(
[ INFO : 2024-09-25 10:32:54,818 ] -                   (dropout_module): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,818 ] -                   (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                   (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,818 ] -                   (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,819 ] -                   (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,819 ] -                   (grep_linear): Linear(in_features=64, out_features=8, bias=True)
[ INFO : 2024-09-25 10:32:54,819 ] -                 )
[ INFO : 2024-09-25 10:32:54,819 ] -                 (dropout1): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,819 ] -                 (dropout2): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,819 ] -                 (dropout3): Dropout(p=0.0, inplace=False)
[ INFO : 2024-09-25 10:32:54,819 ] -                 (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,819 ] -                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)
[ INFO : 2024-09-25 10:32:54,819 ] -                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)
[ INFO : 2024-09-25 10:32:54,819 ] -                 (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,819 ] -               )
[ INFO : 2024-09-25 10:32:54,819 ] -             )
[ INFO : 2024-09-25 10:32:54,819 ] -             (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,819 ] -           )
[ INFO : 2024-09-25 10:32:54,819 ] -           (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
[ INFO : 2024-09-25 10:32:54,819 ] -         )
[ INFO : 2024-09-25 10:32:54,819 ] -       )
[ INFO : 2024-09-25 10:32:54,819 ] -     )
[ INFO : 2024-09-25 10:32:54,819 ] -     (featurizer): Featurizer()
[ INFO : 2024-09-25 10:32:54,819 ] -   )
[ INFO : 2024-09-25 10:32:54,819 ] -   (projection): ArcMarginProduct_intertopk_subcenter(in_features=192, out_features=17982, scale=32.0, margin=0.0, easy_margin=False,K=3, mp=0.06, k_top=5, do_lm=False)
[ INFO : 2024-09-25 10:32:54,819 ] - )
[ INFO : 2024-09-25 10:32:54,819 ] - start_epoch: 1
[ INFO : 2024-09-25 10:32:55,021 ] - <== Loss ==>
[ INFO : 2024-09-25 10:32:55,021 ] - loss criterion is: CrossEntropyLoss
[ INFO : 2024-09-25 10:32:55,023 ] - <== Optimizer ==>
[ INFO : 2024-09-25 10:32:55,023 ] - optimizer is: SGD
[ INFO : 2024-09-25 10:32:55,023 ] - <== Scheduler ==>
[ INFO : 2024-09-25 10:32:55,023 ] - scheduler is: ExponentialDecrease
[ INFO : 2024-09-25 10:32:55,023 ] - <== MarginScheduler ==>
[ INFO : 2024-09-25 10:32:55,029 ] - <========== Training process ==========>
[ INFO : 2024-09-25 10:32:55,029 ] - +----------+----------+----------+----------+----------+----------+
[ INFO : 2024-09-25 10:32:55,029 ] - |     Epoch|     Batch|        Lr|    Margin|      Loss|       Acc|
[ INFO : 2024-09-25 10:32:55,029 ] - +----------+----------+----------+----------+----------+----------+
Traceback (most recent call last):
  File "/home/users/astar/bmsi/liut1/wespeaker_0924/examples/voxceleb/v2/wespeaker/bin/train.py", line 254, in <module>
    fire.Fire(train)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/users/astar/bmsi/liut1/wespeaker_0924/examples/voxceleb/v2/wespeaker/bin/train.py", line 227, in train
    run_epoch(train_dataloader,
  File "/home/users/astar/bmsi/liut1/wespeaker_0924/wespeaker/utils/executor.py", line 48, in run_epoch
    features, _ = model.module.frontend(wavs, wavs_len)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/users/astar/bmsi/liut1/wespeaker_0924/wespeaker/frontend/s3prl.py", line 82, in forward
    feats, feats_lens = self.upstream(input, input_lengths)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/s3prl/nn/upstream.py", line 209, in forward
    hidden_states = self.upstream(wavs_list)["hidden_states"]
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/s3prl/upstream/interfaces.py", line 103, in __call__
    result = super().__call__(wavs, *args, **kwargs) or {}
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/s3prl/upstream/wavlm/expert.py", line 83, in forward
    features, feat_padding_mask = self.model.extract_features(
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/s3prl/upstream/wavlm/WavLM.py", line 361, in extract_features
    features = self.feature_extractor(source)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/s3prl/upstream/wavlm/WavLM.py", line 525, in forward
    x = conv(x)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/s3prl/upstream/wavlm/modules.py", line 36, in forward
    output = F.layer_norm(
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.38 GiB (GPU 0; 39.56 GiB total capacity; 25.55 GiB already allocated; 6.26 GiB free; 32.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 944360) of binary: /home/users/astar/bmsi/liut1/.conda/envs/wespeaker/bin/python
Traceback (most recent call last):
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.13.0', 'console_scripts', 'torchrun')())
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/users/astar/bmsi/liut1/.conda/envs/wespeaker/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
wespeaker/bin/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-25_10:35:00
  host      : x1000c0s2b0n1.hostmgmt2000.cm.asp2a.nscc.sg
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 944360)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
